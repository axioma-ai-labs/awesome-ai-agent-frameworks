<h1 align="center">
	‚ö° Awesome AI Agent Frameworks ‚ö°
</h1>

> Last updated: 2025-09-02

## Description
This repository lists AI Agent Frameworks we‚Äôve battle-tested and/or reviewed while building production-grade systems for 40+ clients. This list is sorted based on our preferability, focuses on real trade-offs, what each framework is **REALLY** good at, and when to reach for it. 

## How we evaluate?
- Reliability: state management, retries, determinism, type safety
- Ergonomics: DX, tooling, validation, tracing, testability, etc.
- Deployment: ease of deployment, observability, failure recovery
- Ecosystem fit: language support, community, ease of maintainability

---

## Table of Contents
1. ü•á [Pydantic AI](#pydantic)
2. ü•à [OpenAI Agents SDK](#openai-agents)
3. üèÖ [CrewAI](#crewai)
4. [LlamaIndex Agents](#llamaindex)
5. [LangGraph](#langgraph)
6. [AutoGen](#autogen)
7. [Haystack Agents](#haystack)
8. [Other Frameworks](#otherframeworks)

---

### ü•á 1. [Pydantic AI](https://ai.pydantic.dev/) <a name="pydantic"></a>

Type-first AI agent framework with tools, validators, provider-agnostic (you can use different LLMs from different providers!), optional graph-style control, and first-class observability via Logfire ‚Äî built around reliable structured (and unstructured) outputs.

**Key Facts**
- Open Source: ‚úÖ Yes
- Language: Python

**Why we like it?** (from our production experience)
- Type safety + validators = dramatically fewer runtime surprises and cleaner contracts between agents and tools
- Logfire integration gives real-time traces, usage, and failure context that feel native‚Äîno duct tape dashboards
- Shareable, controllable context primitives cut weeks of ad-hoc ‚Äúmemory/utils‚Äù code
- Multi-LLM support that actually ships: OpenAI, Anthropic, Gemini, Groq, Mistral, Cohere, Ollama (we‚Äôve mixed and matched)
- FastAPI-inspired design‚Äîminimal, predictable, easy to onboard new engineers
- Tool swapping/mocking is fast, which speeds up testing, troubleshooting, and vendor changes
- Optional graph support when you need explicit plan ‚Üí act ‚Üí observe control without giving up the simple core
- Modularity by default: small typed components that other devs can read, test, and fix

**Where it shines?**
- Production agents that transform/aggregate data or orchestrate many tools with strict output requirements
- Multi-provider setups that need straightforward model swaps/fallbacks without rewriting orchestration
- Teams that care about traceability and cost control (Logfire) for audits, regressions, and SLA debugging

**Takeaways**
1. Treat your Pydantic models as the contract‚Äîdesign them before prompts and tools
2. Start with a tight output schema + validators; add tools only after the contract stabilizes
3. Keep tools/helper functions small, typed, and mockable; write unit tests against the models and tool I/O
4. Turn on Logfire on day one to capture traces/tokens/failures‚Äîpays off immediately as you scale (we run 80+ tools in a multi-agent system)
5. Use context primitives instead of custom ‚Äúmemory managers‚Äù; reach for the graph only when the flow truly needs explicit control


---

### ü•à 2. [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) <a name="openai-agents"></a>

Lightweight, production-minded agent orchestration with a tiny set of primitives (Agents, Tools, Handoffs, Guardrails, Sessions), built-in tracing, and built-in tools (retrieval, web search, computer-use). Provider-agnostic by design.

**Key Facts**
- Open Source: ‚úÖ Yes
- Languages: Python, JS/TS

**Why we like it?**
- Small surface area yet expressive ‚Äî we ship fast and keep orchestration readable as complexity grows
- Handoffs and ‚Äúagents-as-tools‚Äù make multi-agent designs practical without over-engineering
- Built-in traces + usage metrics help us catch regressions early and control cost from day one
- Sessions remove boilerplate memory handling for multi-turn apps
- Works great on OpenAI stack but stays provider-agnostic if we need to mix in other LLMs

**Where it shines?**
- Production apps that need clear observability/auditing with minimal framework overhead
- Multi-agent workflows (planner ‚Üí specialist ‚Üí reviewer) where explicit handoffs improve SLAs
- Assistants that need out-of-the-box web/doc search or computer-use without building custom tools
- Realtime/voice agents that stream audio, tool calls, and state transitions cleanly

**Takeaways**
1. Start with a single agent and 2‚Äì3 critical tools; enable tracing and add lightweight guardrails for inputs/outputs
2. Use handoffs only when specialization is proven; prefer ‚Äúagents as tools‚Äù for modular skills you can swap in/out
3. Persist context with Sessions & enforce token budgets and log usage to keep costs predictable
4. If you must be multi-provider, keep model config abstracted ‚Äî the SDK allows it, but ofc they work better with the OpenAI stack


---

### üèÖ 3. [CrewAI](https://github.com/crewAIInc/crewAI) <a name="crewai"></a>

Role-based ‚Äúcrews‚Äù of agents that execute well-defined tasks with tools and memory. Two simple process modes (sequential or hierarchical with a manager) keep orchestration understandable while still enabling multi-agent collaboration.

**Key Facts**
- Open Source: ‚úÖ Yes
- Language: Python

**Why we like it?**
- Fast mental model: Agents (roles) + Tasks (goals) + simple process = quick shipping without over-engineering
- Multi-agent by default: ‚Äúmanager + specialists‚Äù maps cleanly to planner ‚Üí doer ‚Üí reviewer patterns
- Solid tool ecosystem: the official tools cover real work (browsering, code exec, RAG-style lookups) so we prototype quickly
- Pragmatic memory options: start with basic memory, opt into longer-term persistence when outcomes justify it

**Where it shines?**
- Product/research teams shipping practical assistants fast (researcher ‚Üí analyst ‚Üí writer)
- Workflows that benefit from a lightweight manager delegating/ordering tasks (hierarchical mode)
- Coding/analysis copilots that need an embedded Python sandbox (Code Interpreter) and web/data tools

**Watch-outs**
- Less low-level control than graph/state-machine frameworks; complex recovery/checkpointing will be DIY
- Hierarchical setups add indirection‚Äîgreat for complexity, but overkill for linear flows
- Long-term memory persistence requires configuration (paths/providers); plan this early to avoid surprises later

**Takeaways**
1. Start sequential with a 3-agent crew (researcher ‚Üí analyst ‚Üí writer) and crisp Task specs (objective, inputs, success criteria, expected output)
2. Introduce a manager (hierarchical) only when the task graph truly branches or you need dynamic delegation
3. Leverage the official tools first (browser, code interpreter, docs search); add custom tools only where they beat off-the-shelf ones
4. Enable basic memory for context; adopt long-term/entity memory once you have a retention plan and storage path/provider locked in

---

The following list below is sorted by quality, ease of use, and production-readiness based on our experience. However, all the AI agent frameworks mentioned are just as impressive as the Top 3 and have the potential to reach the top as well!

---

### 4. [LlamaIndex Agents](https://docs.llamaindex.ai/) <a name="llamaindex"></a>

Data-centric agents built on top of strong RAG primitives. You get clean agent abstractions (FunctionAgent / ReAct-style), a robust Tool system, memory, and an ecosystem of connectors via LlamaHub. For heavier workflows, LlamaIndex adds event-driven ‚ÄúWorkflows‚Äù, plus managed services like LlamaCloud and LlamaParse for high-fidelity document parsing and ingestion

**Key Facts**
- Open Source: ‚úÖ Yes (MIT)
- Languages: Python, TS
- Building blocks: Agents (FunctionAgent/ReAct), Tools & ToolSpecs, Memory, Query Engines/Retrievers, Workflows, Observability; optional LlamaCloud and LlamaParse

**Why we like it?**
- Good data connectors + loaders via LlamaHub; Good to wire enterprise sources fast without custom glue
- Clean upgrade path: start with a RAG app (indexes, retrievers), then wrap it in an agent as the use case matures
- Tools are first-class (and typed), so agents can call query engines, functions, or external APIs in a predictable way
- Workflows cover the ‚Äúagentic but reproducible‚Äù gap‚Äîgreat for multi-step plans, routing, and custom orchestration


**Where it shines?**
- Knowledge assistants over heterogeneous corpora where retrieval quality, routing, and synthesis matter (reports, SOPs, tech docs)
- Multi-document agents that plan, rerank, and call specialized tools for doc-specific questions
- End-to-end ‚Äúagentic document workflows‚Äù that combine parsing, retrieval, structured outputs, and orchestration for knowledge work

**Good starting point**
1. Build your index + retriever and validate retrieval quality in isolation (small gold set + evals)
2. Expose 2‚Äì3 high-value Tools (query engine, web/doc fetch, domain API). Keep tool I/O narrow and typed
3. If docs are messy (tables, figures, scans), introduce LlamaParse to improve chunk fidelity before tuning prompts
4. For branching plans or multi-stage jobs, move the core into a Workflow and keep the agent as the front-door

---

### 5. [LangGraph](https://github.com/langchain-ai/langgraph) <a name="langgraph"></a>

Graph/state-machine approach for building stateful, recoverable agent loops with optional checkpointing and persistence. Great when auditability matter more than ‚Äúauto‚Äù magic. Quite heavy!

**Key Facts**
- Open Source: ‚úÖ Yes
- Languages: Python, JS/TS

**Why we like it?** 
- Precise control over tool-use loops, branching, and backoff/retry strategies
- Checkpoint + resume for long-running tasks and robust failure recovery
- Clean separation of nodes keeps complex flows readable and testable

**Where it shines?**
- Regulated or high-SLA workflows that need traceability and resumability
- Multi-step retrieval/decision pipelines (plan ‚Üí act ‚Üí observe) with human approval gates

**Watch-outs**
- Memory semantics are nontrivial
- Often version shifts introduce notable API changes (plan migrations intentionally!)
- Heavy multi-agent abstractions (very difficult to keep up)


---

### 6. [AutoGen](https://microsoft.github.io/autogen/) <a name="autogen"></a>

Event-driven multi-agent framework where agents (human, assistant, tool, evaluator) converse, collaborate, and call tools. Includes patterns like GroupChat and a visual Studio for orchestration and inspection.

**Key Facts**
- Open Source: ‚úÖ Yes
- Primary Language: Python (with a .NET variant available)
- Core concepts: AgentChat APIs (`ConversableAgent`, `AssistantAgent`, `UserProxyAgent`), GroupChat with a `GroupChatManager`, tool/function calling, optional code execution via the user proxy agent

**Why we like it?**
- Rich multi-agent patterns out of the box (planner/solver/reviewer, debate, supervisor) without re-implementing orchestration
- Studio trims debugging time by making agent interactions observable and tweakable
- Easy to add human-in-the-loop checkpoints exactly where decisions matter (e.g., before execution or external calls)
- ‚ÄúAgents as chat participants‚Äù is a mental model that scales from two roles to larger teams cleanly

**Where it shines?**
- Research & prototyping of collaborative behaviors and role specialization
- Complex tasks that benefit from peer-review or committee-style decision making
- Teams that want a GUI (Studio) to wire skills/tools and iterate quickly with stakeholders

**Watch-outs**
- Heavier footprint vs. minimal SDKs; expect more moving parts and some learning curve
- Often version shifts introduce notable API changes (plan migrations intentionally!)

---

### 7. [Haystack Agents](https://docs.haystack.deepset.ai/) <a name="haystack"></a>

**Key Facts**
- Open Source: ‚úÖ Yes
- Language: Python

**Why we like it?**
- Mature retrieval/search stack and clean integration between RAG components and the agent loop
- ‚ÄúExit conditions‚Äù make agent behavior predictable‚Äîgreat for SLAs and cost control
- Strong production story: tracing, logging, and cloud-native deployment paths are documented

**Where it shines?**
- Enterprise RAG with observability and on-prem/cloud portability (vector stores, retrievers, rankers, tools)

**Watch-outs:**
- Can be verbose - start with a small set of components and add conditional routing only when needed (agentic pipelines)
- Quite difficult to maintain
- Very easy to over-engineer and hard to read if you add too many nodes early


---

### 8. Other Frameworks <a name="otherframeworks"></a>

The frameworks below were **NEVER** tested by us, but they've been on our radar and timeline quite often, so maybe you can check them out first :)

- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) ‚Äî Classic autonomous agent project; great for experiments and understanding long-running loops. Ideal for self-contained tasks like market research, but watch for potential API cost overruns.
- [MetaGPT](https://github.com/FoundationAgents/MetaGPT) ‚Äî Multi-agent ‚Äúcompany-as-code‚Äù pattern with research-friendly team roles (PM/Eng/QA). Excels in simulating collaborative workflows for prototyping complex systems.
- [Camel-AI](https://github.com/camel-ai/camel) ‚Äî Role-playing agents with conversation curricula for cooperative task-solving. Strong for research-driven scenarios requiring structured multi-agent interactions.
- [OpenHands](https://github.com/All-Hands-AI/OpenHands) ‚Äî Computer-use/desktop-control agents for browser and OS actions. Perfect for grounded tasks like automating desktop workflows or testing UI interactions.
- [AgentScope](https://github.com/agentscope-ai/agentscope) ‚Äî Alibaba‚Äôs framework for building and evaluating scalable multi-agent systems. Suited for large-scale, enterprise-grade applications with robust evaluation needs.
- [LangChain](https://www.langchain.com/) ‚Äî Modular framework for building LLM applications with chains, tools, and agents. Shines in rapid prototyping and integrations; our go-to for 50+ client projects needing flexible data pipelines.
- [AgentFlow](https://github.com/lebrunel/agentflow) ‚Äî Low-code framework using Markdown and natural language for AI-powered workflows. Simplifies agent creation for quick automation; ideal for non-technical teams.
- [Atomic Agents](https://github.com/BrainBlend-AI/atomic-agents) ‚Äî Lightweight, modular framework for building agentic AI pipelines with atomic components. Great for developers prioritizing simplicity and composability in smaller projects.
- [PromptFlow](https://github.com/microsoft/promptflow) ‚Äî Microsoft‚Äôs tool for developing and evaluating LLM-based flows. Excels in structured prompt engineering and testing for enterprise-grade applications.
- [n8n](https://n8n.io/) ‚Äî Workflow automation platform with AI capabilities for integrating agent-like tasks. Perfect for connecting APIs and automating business processes with minimal coding.
- [OpenAgents](https://github.com/xlang-ai/OpenAgents) ‚Äî Open platform for hosting and using language agents in real-world scenarios. Strong for community-driven, extensible agent deployments.
- [Lindy](https://www.lindy.ai/) ‚Äî AI agent platform for creating and managing business agents with simple prompts. Ideal for startups needing quick, user-friendly automation solutions.
